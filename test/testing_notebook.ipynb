{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GItTOeDLXRKj"
      },
      "source": [
        "Standard imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMxpmb9SXKqM",
        "outputId": "b0ee8446-a384-4f1d-f9c4-ed0e22198016"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJQroJ_jFPUs",
        "outputId": "f962b3d1-d656-4fd0-9ae3-c10f51ca7c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Module path: /usr/local/lib/python3.11/dist-packages/fitz/__init__.py\n",
            "✅ Has fitz.open(): True\n"
          ]
        }
      ],
      "source": [
        "# import fitz\n",
        "# print(\"✅ Module path:\", fitz.__file__)\n",
        "# print(\"✅ Has fitz.open():\", hasattr(fitz, \"open\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA-DxOsVXUax"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ55MLNwXXCJ"
      },
      "outputs": [],
      "source": [
        " # !pip install -qU \"langchain[anthropic]\"\n",
        "# !pip install -qU langchain-huggingface\n",
        "# !pip install langchain_community\n",
        "# !pip install langgraph\n",
        "# !pip install pypdf\n",
        "# !pip3 install tools\n",
        "# !pip3 install fitz\n",
        "# !pip install pymupdf\n",
        "# !pip3 install PyPDF2\n",
        "# !pip3 install fpdf\n",
        "# !pip3 install rake_nltk\n",
        "#!pip install matplotlib\n",
        "# !pip install --force-reinstall pymupdf \n",
        "# !pip3 install rapidfuzz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0jr8gEZXaY6"
      },
      "source": [
        "Chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1eDlw5gW00y",
        "outputId": "682364b3-af5b-4ace-e6cc-c70192c13979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Anthropic: ··········\n"
          ]
        }
      ],
      "source": [
        "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
        "  os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoiiuHAFXdl_"
      },
      "source": [
        "Embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEGjdpltXf1U",
        "outputId": "58776ba9-e9c6-40b4-b5fd-9ffabdab9456"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZA690KXzhA"
      },
      "source": [
        "Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3X3D5Is5X13P"
      },
      "outputs": [],
      "source": [
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbxdAIiRYNAl"
      },
      "source": [
        "RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "WSM0i9ymYOp6"
      },
      "outputs": [],
      "source": [
        "# Load and chunk contents of the blog\n",
        "file_path = \"/content/sample_demo_file_Extracted.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size= 600, chunk_overlap= 100)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
        "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "# 1. Define a better, strict RAG-style prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use the context below to answer the question using only the information found in the context.\n",
        "\n",
        "Respond with a short phrase, one or two words, or a single sentence at most.\n",
        "Do not explain or generalize. Keep your answer brief and factual.\n",
        "\n",
        "---\n",
        "\n",
        "Example:\n",
        "\n",
        "Context:\n",
        "The patient is a 59-year-old female with a history of hypothyroidism and insomnia.\n",
        "\n",
        "Question:\n",
        "What is the age of the patient?\n",
        "\n",
        "Answer:\n",
        "59-year-old\n",
        "\n",
        "---\n",
        "\n",
        "Now answer the following:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "# 2. Everything else stays the same\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.format_messages(\n",
        "        question=state[\"question\"],\n",
        "        context=docs_content\n",
        "    )\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content.strip(),\n",
        "            \"context_texts\": [doc.page_content for doc in state[\"context\"]]\n",
        "     }\n",
        "\n",
        "# 3. Build your graph as before\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYVAzc1uYVwj",
        "outputId": "4cfab5ea-a126-4ef2-9b0d-64f285d6f7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hematomas\n"
          ]
        }
      ],
      "source": [
        "response = graph.invoke({\"question\": \"What did MRI results reveal? \"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_kqT_FUSohB",
        "outputId": "6490a39f-28b0-4195-84de-6b65e3f7622f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hematomas. This was found on MRI of brain following a head trauma 2 months prior. Patient had been having\n"
          ]
        }
      ],
      "source": [
        "print(response['context'][2].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "Ys1zNqxNRbmr"
      },
      "outputs": [],
      "source": [
        "# print(response['context_texts'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJvjqafxcP7R"
      },
      "source": [
        "Finding location in document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "d3tqu-VToICR"
      },
      "outputs": [],
      "source": [
        "answer = response['context'][2].page_content\n",
        "# case_sensitive = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-xIWMipOBbH",
        "outputId": "73e0b63f-77ff-4b2b-ed53-51fb3fee48ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hematomas. This was found on MRI of brain following a head trauma 2 months prior. Patient had been having\n"
          ]
        }
      ],
      "source": [
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yhgKYeKORVg",
        "outputId": "6ebc6a5c-8b28-46bc-d210-1053099fcafb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Ijf1UESZrCNl"
      },
      "outputs": [],
      "source": [
        "input_pdf_path = file_path\n",
        "output_pdf_path = 'output_bold.pdf'\n",
        "# target_string = answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4UKmYizs05T"
      },
      "source": [
        "Highlight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "8HcWLQeGT6TP"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/sample_demo_file_Extracted.pdf\"\n",
        "import fitz\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "zSiOoTfZ5liN"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "def find_best_match(answer, page, threshold=85):\n",
        "    blocks = page.get_text(\"blocks\")  # Each block = (x0, y0, x1, y1, text, block_no, block_type, ...)\n",
        "    for block in blocks:\n",
        "        x0, y0, x1, y1, text, *_ = block\n",
        "        score = fuzz.partial_ratio(answer.lower(), text.lower())\n",
        "        if score >= threshold:\n",
        "            return fitz.Rect(x0, y0, x1, y1)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "eQ_i9aGf6f29"
      },
      "outputs": [],
      "source": [
        "def highlight_answer_with_fuzzy_search(input_pdf_path, output_pdf_path, answer, threshold=85):\n",
        "    doc = fitz.open(input_pdf_path)\n",
        "    found = False\n",
        "\n",
        "    for page in doc:\n",
        "        rect = find_best_match(answer, page, threshold)\n",
        "        if rect:\n",
        "            page.add_highlight_annot(rect)\n",
        "            found = True\n",
        "            print(f\"✅ Match found and highlighted on page {page.number + 1}\")\n",
        "        else:\n",
        "            print(f\"⚠️ No match found on page {page.number + 1}\")\n",
        "\n",
        "    if found:\n",
        "        doc.save(output_pdf_path)\n",
        "        print(f\"✅ Highlighted PDF saved to: {output_pdf_path}\")\n",
        "    else:\n",
        "        print(\"❌ No match found in entire PDF.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjOlMCDU5n5n",
        "outputId": "3ac6a75c-4747-4ef7-c876-67a17f451869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Match found and highlighted on page 1\n",
            "⚠️ No match found on page 2\n",
            "⚠️ No match found on page 3\n",
            "⚠️ No match found on page 4\n",
            "⚠️ No match found on page 5\n",
            "⚠️ No match found on page 6\n",
            "⚠️ No match found on page 7\n",
            "✅ Highlighted PDF saved to: output.pdf\n"
          ]
        }
      ],
      "source": [
        "highlight_answer_with_fuzzy_search(file_path, \"output.pdf\", response['context'][2].page_content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
